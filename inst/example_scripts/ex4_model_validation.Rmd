---
title: "REMA Model Validation"
author: "LJ Balstad"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{REMA basics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include=FALSE, echo=FALSE, results='hide', fig.keep='all'}

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
rema_path <- find.package("rema")
knitr::opts_knit$set(root.dir = file.path(rema_path, "example_data"))
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# devtools::build_rmd(files = 'vignettes/ex5_model_validation.Rmd')

```

```{r setup, warning = FALSE, message = FALSE, echo=FALSE,results='hide',fig.keep='all'}

library(rema)
library(knitr)

# or just load tidyverse
library(ggplot2)
library(readr)
library(dplyr)
library(tidyr)

# Packages for MCMC diagnostics
library(tmbstan) 
# tmbstan relies on rstan, which now needs to be installed through the R
# universe. if you are experiencing errors with MCMC, try uninstalling and then
# re-installing rstan using this code chunk (also don't forget to restart your
# computer so that your path resets)
# install.packages("rstan", repos = c("https://stan-dev.r-universe.dev",
#                                     "https://cloud.r-project.org"))
library(PerformanceAnalytics) 

ggplot2::theme_set(cowplot::theme_cowplot(font_size = 14) +
                     cowplot::background_grid() +
                     cowplot::panel_border())

```

### A testing framework for REMA model validation

In this vignette, we will be testing that (1) the REMA model is working as expected (i.e., code has been implemented correctly and parameters are estimated without bias), and (2) the assumptions made when parameterizing and estimating the REMA model are valid, including assumptions related to random effects and error structure. We use two example stocks with different levels of complexity:

-   Aleutian Islands Pacific cod (AI Pcod; Spies et al., 2023): a simple, univariate case, which fits to a single time series of the AI bottom trawl survey and estimates one process error

-   Gulf of Alaska Thornyhead rockfish (GOA thornyhead; Echave et al., 2022): a complex, multi-strata and multi-survey case, which estimates multiple process errors, a scaling parameter for the longline survey, and two additional observation errors for the GOA bottom trawl and longline surveys

Testing the REMA model across this range of complexity helps ensure that model and model assumptions are valid in a variety of realistic, management-relevant cases.

It is important to note that model validation does not mean the model is "more right" or "more correct." Model validation does not help an analyst with model selection, except to identify models which are not functioning properly, nor does it ensure that the model makes "better" predictions. Rather, model validation is a way to ensure that the model is operating as expected, without introducing bias or violating statistical assumptions upon which the model is based, and (Auger‐Méthé et al., 2016; Auger‐Méthé et al., 2021).

The key questions we aim to answer are:

-   Does the model introduce bias? Using a simulation self-check, we will test if the model is coded correctly and is able to recover known parameters.
-   Is it reasonable that the data could have been generated by the model? Using one-step ahead (OSA) residuals, the appropriate residual type for state-space models, we test the model assumptions and look for trends in residuals that may reveal characteristics or dynamics of the data that aren't adequately captured by the model.
-   Are the normality assumptions made when estimating random effects via the Laplace approximation valid? By comparing a non-Laplace approximation and a Laplace approximation of the model via MCMC, we test if the distribution of the fixed effects parameters' posteriors are normal or normal-like.
-   Are the parameters unique and non-redundant? Checking the correlation between parameters helps us identify if parameters are redundant with each other.
-   Is the model identifiable? Using a likelihood profiling approach, we will test if model parameters are identifiable, which means they have a solution (i.e., a minimum log-likelihood) and aren't highly correlated with another parameter

### Prepare data for each stock here

First, we run the model for each stock. The AI Pcod model (pcod_mod) uses a single process error to describe the stock over time (one parameter). The GOA thornyhead rockfish model (thrn_mod) uses three process errors and data from two surveys (bottom trawl and longline survey) to describe the stock over time (six parameters; three sur).

```{r data and model prep, fig.keep='all', message=FALSE, warning=FALSE, results='hide'}

set.seed(415) # for repetability, use a seed

# first, get the p-cod data set up
# pcod_bio_dat <- read_csv("inst/example_data/ai_pcod_2022_biomass_dat.csv")
pcod_bio_dat <- read_csv("../example_data/ai_pcod_2022_biomass_dat.csv")
pcod_input <- prepare_rema_input(model_name = "p_cod",
                                 biomass_dat = pcod_bio_dat,
                                 # one strata
                                 PE_options = list(pointer_PE_biomass = 1)
                                 )
# run the model
pcod_mod <- fit_rema(pcod_input) 

# next, get the thornyhead data set up
# thrn_bio_dat <- read_csv("inst/example_data/goa_thornyhead_2022_biomass_dat.csv")
thrn_bio_dat <- read_csv("../example_data/goa_thornyhead_2022_biomass_dat.csv")
# thrn_cpue_dat <- read_csv("inst/example_data/goa_thornyhead_2022_cpue_dat.csv")
thrn_cpue_dat <- read_csv("../example_data/goa_thornyhead_2022_cpue_dat.csv")
thrn_input <- prepare_rema_input(model_name = 'thrnhead_rockfish',
                                multi_survey = TRUE,
                                biomass_dat = thrn_bio_dat,
                                cpue_dat = thrn_cpue_dat,
                                # RPWs are a summable/area-weighted effort index
                                sum_cpue_index = TRUE, 
                                # three process error parameters (log_PE) estimated,
                                # indexed as follows for each biomass survey stratum
                                # (shared within an area across depths):
                                PE_options = list(pointer_PE_biomass = c(1, 1, 1, 2, 2, 2, 3, 3, 3)),
                                # scaling parameter options:
                                q_options = list(
                                # longline survey strata (n=3) indexed as follows for the
                                # biomass strata (n=9)
                                pointer_biomass_cpue_strata = c(1, 1, 1, 2, 2, 2, 3, 3, 3),
                                # one scaling parameters (log_q) estimated, shared
                                # over all three LLS strata
                                pointer_q_cpue = c(1, 1, 1)),
                                # estimate extra trawl survey observation error
                                extra_biomass_cv = list(assumption = 'extra_cv'),
                                # estimate extra longline survey observation error
                                extra_cpue_cv = list(assumption = 'extra_cv')
                                )

# run the model
thrn_mod <- fit_rema(thrn_input) 

```

### 1. Simulation self-check: Can we recover parameters without bias?

Simulation self-checking is an important step to ensure the model has been properly coded and is consistent in its running [1, 3]. First, we use the REMA model to estimate parameters (e.g., process error variance) from real data. Next, we use the estimated parameters to simulate new data (simulated observations and states) using the REMA equations. Finally, we use the REMA model to recover the parameters from the simulated data, which has a known process error variance.

Models fail simulation testing when the recovered parameters or simulated state spaces deviate consistently from the true values used to simulate data. If the recovered parameters are far from the true process error or the span of the recovered parameters is large, this indicates that the model is non-identifiable, has redundant parameters, or has bias.

```{r simulation testing function, echo=TRUE, warning=FALSE, message=FALSE}

# this will run sim testing
# mod_name (variable name) gives the model which should be used
# replicates (numeric) gives the number of simulations to complete
# cpue (T/F) is TRUE if there is CPUE data and FALSE if there is not CPUE data
# runs simulations
# returns estimated parameters as a matrix, with each column being a parameter and each row being an iteration 
sim_test <- function(mod_name, replicates, cpue) {
  
  # storage things
  re_est <- matrix(NA, replicates, length(mod_name$par))
  
  # go through the model
  suppressMessages(for(i in 1:replicates) {
    
    sim <- mod_name$simulate(complete = TRUE) # simulates the data
    
    # use biomass from simulation...
    tmp_biomass <- matrix(data = exp(sim$log_biomass_obs), ncol = ncol(mod_name$input$data$biomass_obs))
    colnames(tmp_biomass) <- colnames(mod_name$data$biomass_obs)
    # and if there's cpue data, add that too ...
    # if (cpue) {tmp_cpue <- matrix(data = exp(sim$log_cpue_obs), ncol = ncol(mod_name$input$data$cpue_obs))
    if (cpue) {tmp_cpue <- matrix(data = sim$cpue_obs, ncol = ncol(mod_name$input$data$cpue_obs))
    colnames(tmp_cpue) <- colnames(mod_name$data$cpue_obs)}
    # ... to set up new data for input
    newinput <- mod_name$input
    newinput$data$biomass_obs <- tmp_biomass # biomass data
    if (cpue) {newinput$data$cpue_obs <- tmp_cpue} # cpue data
    
    # obsvec used for osa residual calculations and more specifically is the
    # observation vector used in the likelihood functions. need to transpose
    # these matrices in order to get the order correct (by row instead of by
    # column...)
    newinput$data$obsvec <- t(sim$log_biomass_obs)[!is.na(t(sim$log_biomass_obs))] 
    if (cpue) {newinput$data$obsvec <- c(t(sim$log_biomass_obs)[!is.na(t(sim$log_biomass_obs))], t(sim$log_cpue_obs)[!is.na(t(sim$log_cpue_obs))])} 
    
    # refit model 
    mod_new <- fit_rema(newinput, do.sdrep = FALSE)
    
    # add parameter estimates to matrix
    re_est[i, ] <- mod_new$env$last.par[1:length(mod_name$par)]

    })
  
  re_est <- as.data.frame(re_est); re_est$type <- rep("recovered")

  return(re_est)
  
}
# sometimes spits out: In stats::nlminb(model$par, model$fn, model$gr, control = list(iter.max = 1000,...: NA/NaN function evaluation

# run for pcod and prep data frame
# run simulation testing
par_ests <- sim_test(mod_name = pcod_mod, replicates = 250, cpue = FALSE)
# get data frame with simulation values for each parameter
mod_par_ests <- data.frame("log_PE1" = pcod_mod$env$last.par[1], 
                       type = "model")
names(par_ests) <- names(mod_par_ests) # rename for ease
pcod_par_ests <- rbind(mod_par_ests, par_ests) # recovered and model in one data frame
pcod_par_ests$sp <- rep("AI Pcod")

# run for thorny and prep data frame -- same process
# run simulation testing
par_ests <- sim_test(mod_name = thrn_mod, replicates = 250, cpue = TRUE) # note some warnings
# get data frame with simulation values for each parameter
mod_par_ests <- data.frame("log_PE1" = thrn_mod$env$last.par[1],
                           "log_PE2" = thrn_mod$env$last.par[2], 
                           "log_PE3" = thrn_mod$env$last.par[3], 
                           "log_q" = thrn_mod$env$last.par[4], 
                           "log_tau_biomass" = thrn_mod$env$last.par[5], 
                           "log_tau_cpue" = thrn_mod$env$last.par[6], 
                           type = "model")
names(par_ests) <- names(mod_par_ests) # rename for ease
thrn_par_ests <- rbind(mod_par_ests, par_ests) # recovered and model parameters in one data frame
thrn_par_ests$sp <- rep("GOA Thornyhead")

```

```{r simulation testing plot, echo=TRUE, warning=FALSE,  message=FALSE}

# reogranize data
pcod_tmp <- pcod_par_ests %>% pivot_longer(1, names_to = "parameter")
thrn_tmp <- thrn_par_ests %>% pivot_longer(1:6, names_to = "parameter")
dat_tmp_par <- rbind(thrn_tmp, pcod_tmp)

# plotting parameter number v. value
ggplot(NULL, aes(parameter, value)) + 
  # add distribution of recovered parameters
  geom_violin(data = dat_tmp_par %>% filter(type == "recovered"), 
              fill = "#21918c", draw_quantiles = c(0.5)) +
  # add "true values" from original model
  geom_point(data = dat_tmp_par %>% filter(type == "model"), 
             size = 3, col = "black") + 
  # seperate by species 
  facet_grid(. ~ sp, scales = "free", space = "free") + 
  # make cute
  labs(x = "Parameter", 
       y = "Process error (log scale)") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

**Fig 1: Simulation testing, parameter outcomes.** The distribution of recovered parameters is given in teal, with medians marked by horizontal black lines, and the model parameters are given by the black dots. The Pacific cod model and simulation testing is on the left, and the thornyhead rockfish model and simulation testing is on the right.

We can see that the single process error for Pacific cod model is generally recovered in simulation testing. The median of simulations is similar to the model value, and the spread of the simulations is relatively low (recall this is plotted on the log scale, so long negative tails are less concerning than long positive tails).

The parameters for the thornyhead rockfish model are generally recovered as well. Convergence for process error 3 and q (the catchability rescaling) are consistently recovered close to the true (simulated value). There is some variation in recovering the remaining parameters. While the spread is high, and does have a longer positive tail (above the true, simulated value), most distribution is centered on the true, simulated values and there is no clear bias for any parameter.

### 2. Likelihood profile: Is the model identifible?

A likelihood profile is one way to visualize model identifiablility [1, 3, 4]. A model is identifiable if there is a particular value of a parameter which most increases the model's log-likelihood (when all other parameters are held constant) compared to other values of the parameter of interest. For the Pacific cod model, this means testing the model's log-likelihood as the process error parameter changes. For the thornyhead rockfish model, we calculate the log-likelihood as just one parameter changes, when all other parameters are held constant.

Ideally, we want the model's likelihood profile to be smooth with a single peak; this indicates that there is a local maximum the model consistently converges towards.

```{r calculate and plot likelihood profiles, echo=FALSE, message=FALSE, warning=FALSE, results='hide',fig.keep='all', fig.height=4, fig.width=10}

# for pcod, one parameter
pcod_profs <- TMB::tmbprofile(pcod_mod, lincomb = c(1)); pcod_profs$name <- rep("log_PE1")
pcod_profs$sp <- rep("AI Pcod")

# for thornyhead, have 6 parameters. use lincomb to indicate which parameter we are interested in (1) and which parameters to hold constant (0)
thrn_prof_pe1 <- TMB::tmbprofile(thrn_mod, lincomb = c(1, 0, 0, 0, 0, 0)); thrn_prof_pe1$name <- rep("log_PE1")
thrn_prof_pe2 <- TMB::tmbprofile(thrn_mod, lincomb = c(0, 1, 0, 0, 0, 0)); thrn_prof_pe2$name <- rep("log_PE2")
thrn_prof_pe3 <- TMB::tmbprofile(thrn_mod, lincomb = c(0, 0, 1, 0, 0, 0)); thrn_prof_pe3$name <- rep("log_PE3")
thrn_prof_q <- TMB::tmbprofile(thrn_mod, lincomb = c(0, 0, 0, 1, 0, 0)); thrn_prof_q$name <- rep("log_q")
thrn_prof_t_bio <- TMB::tmbprofile(thrn_mod, lincomb = c(0, 0, 0, 0, 1, 0)); thrn_prof_t_bio$name <- rep("log_tau_biomass")
thrn_prof_t_cpue <- TMB::tmbprofile(thrn_mod, lincomb = c(0, 0, 0, 0, 0, 1)); thrn_prof_t_cpue$name <- rep("log_tau_cpue")
thrn_profs <- rbind(thrn_prof_pe1, thrn_prof_pe2, thrn_prof_pe3, thrn_prof_q, thrn_prof_t_bio, thrn_prof_t_cpue)
thrn_profs$sp <- rep("GOA Thornyhead")

profile_dat <- rbind(pcod_profs, thrn_profs)

ggplot(profile_dat, aes(parameter, -value)) + # recall value is negative log-likelihood
  geom_path(lwd = 1.1) +   
  facet_wrap(vars(sp, name), nrow = 1, scales = "free") +
  labs(x = "Parameter value", y = "Log-likelihood") + 
  theme(axis.text = element_text(size = 7), 
        axis.title = element_text(size = 7),
        strip.text = element_text(size = 7))

```

**Fig 2: Conditional log-likelihood profiles.** The log-likelihood (y-axis) is plotted across a range of parameter values for each parameter (x-axis), holding all other values constant. The likelihood profile for the Pacific cod model is on the left, and the likelihood profiles for the thornyhead rockfish model are on the right.

All the model parameters show a single, smooth curve, indicating that the parameters are identifiable, given all other parameters are constant.

### 3. Laplace approximation: Are the model assumptions related to random effects estimation reasonable?

The `rema` library was developed in Template Model Builder (`TMB`), which uses maximum marginal likelihood estimation with the Laplace approximation to efficiently estimate high dimensional, non-linear mixed effects models in a frequentist framework (Skaug and Fournier, 2006; Kristensen et al., 2012; Kristensen, 2018). The primary assumption in models using the Laplace approximation is that the random effects follow a normal distribution. This assumption simplifies the complex integrals that make up the likelihood function. The Laplace approximation is fast and accurate when the normality assumption is met; however, if the true distribution of the random effects is not normal, the Laplace approximation may introduce bias into the parameter estimates.

We can test the validity of the Laplace approximation using Markov chain Monte Carlo (MCMC) sampling, comparing the distributions of the fixed effects parameters from MCMC-sampled models with and without the Laplace approximation (Monnahan and Kristensen, 2018). To do so, we compare the distributions using QQ plots between the two model cases. The Laplace approximation is a reasonable assumption if the sampling quantiles for the two models (with and without the Laplace approximation) are similar, i.e., they fall on the 1:1 line. This can also help us identify bias introduced by the Laplace approximation, for example, if the median (50%) is different when using the Laplace approximation.

The function below is used to run the two model cases:

```{r mcmc laplace approximation test, message=FALSE, warning=FALSE, echo=FALSE,results='hide',fig.keep='all'}

# function to (1) run models and (2) return posterior data frames and the traceplots
# input is model, number of iterations (samples), number of chains
mcmc_comp <- function(mod_name, it_numb, chain_numb) {
  
  # set up MCMC chain information
  it_num <- it_numb
  chain_num <- chain_numb
  
  # run model with laplace approximation
  mod_la <- tmbstan(obj = mod_name, chains = chain_num, init = mod_name$par, laplace = TRUE, iter = it_num)
  # run model without laplace approximation, i.e., all parameters fully estimated without assumptions of normality
  mod_mcmc <- tmbstan(obj = mod_name, chains = chain_num, init = mod_name$par, laplace = FALSE, iter = it_num) 
  
  # posteriors as data frame
  post_la <- as.data.frame(mod_la); post_la$type <- ("la")
  post_mcmc <- as.data.frame(mod_mcmc); post_mcmc <- post_mcmc[, c(1:length(mod_name$par), dim(post_mcmc)[2])]; post_mcmc$type <- rep("mcmc")
  # informational things... this is for getting the posterior draws, i.e., to test traceplots and such
  post_la$chain <- rep(1:chain_num, each = it_num/2)
  post_la$iter_num <- rep(1:(it_num/2), chain_num)
  post_mcmc$chain <- rep(1:chain_num, each = it_num/2)
  post_mcmc$iter_num <- rep(1:(it_num/2), chain_num)
  post_draws <- rbind(post_la, post_mcmc)

  # get quantiles
  qv <- seq(from = 0, to = 1, by = 0.01)
  quant_dat <- data.frame(quant = NULL, 
                          la = NULL, 
                          mcmc = NULL, 
                          par = NULL)
  
  for (i in 1:(dim(post_la)[2]-4)) { # post_la has type, chains, lp, iteration number column that don't count
    
    tmp <- data.frame(quant = qv, 
                      la = quantile(unlist(post_la[i]), probs = qv),
                      mcmc = quantile(unlist(post_mcmc[i]), probs = qv), 
                      par = rep(paste0("V", i)))
    
    quant_dat <- rbind(quant_dat, tmp)
    
  }
  
  return(list(quant_dat, post_draws))
  
}

# run models
pcod_comp <- mcmc_comp(mod_name = pcod_mod, it_numb = 4000, chain_numb = 4)
thrn_comp_log_tau <- mcmc_comp(mod_name = thrn_mod, it_numb = 1500, chain_numb = 2) # running low bc consistently fails test

```

Note standard MCMC diagnostics (e.g., traceplot) can be computed by running the above models outside the \`\`mcmc_comp'' function.

```{r laplace approx plot, echo=FALSE, message=FALSE, warning=FALSE, results='hide',fig.keep='all',  fig.height=4, fig.width=10}

# clean up data frame names by renaming things
pcod_qq <- pcod_comp[[1]]
pcod_qq$par_name <- rep("log_PE1"); pcod_qq$sp <- rep("Pacific cod")
thrn_qq <- thrn_comp_log_tau[[1]]
thrn_qq$par_name <- recode(thrn_qq$par, 
                           V1 = "log_PE1", 
                           V2 = "log_PE2", 
                           V3 = "log_PE3", 
                           V4 = "log_q", 
                           V5 = "log_tau_biomass", 
                           V6 = "log_tau_cpue")
thrn_qq$sp <- rep("thornyhead rockfish")
qq_dat <- rbind(pcod_qq, thrn_qq)

# plot
ggplot(qq_dat, aes(mcmc, la)) + 
  geom_abline(intercept = 0, slope = 1, col = "lightgray") + 
  geom_point() +  
  facet_wrap(~sp + par_name, scales = "free", nrow = 1) + 
  labs(x = "MCMC", y = "Laplace approx.") + 
  theme(axis.text = element_text(size = 7), 
        axis.title = element_text(size = 7),
        strip.text = element_text(size = 7))

```

**Fig 3: Testing the Laplace approximation.** The compared quantiles from full MCMC testing (x-axis) and the Laplace approximation (y-axis) are given with the black points. The gray line is the 1:1 line. Points that fall on the gray line indicate that the quantile value is the same between the two cases.

For the simple Pacific cod model, the Laplace approximation and full MCMC testing show a similar distribution, indicating that the Laplace approximation is reasonable.

For the more complex thornyhead rockfish model, the log_tau_biomass parameter shows significant deviations between the two model cases. Moreover, there are significant unresolved sampling problems of the log_tau_biomass parameter in both cases, indicating that the Laplace approximation might be inappropriate and that further investigation into model structure might be necessary.

### 4. Parameter correlation: Are the model parameters non-redundant?

Parameter redundancy refers to the idea that multiple parameters contribute to the model in the same way. An intuitive case is $y ~ \beta_1 + \beta_2 + \alpha x$, since the model could estimate many combinations of $\beta_1$ and $\beta_2$ which minimize the log-likelihood, i.e., the sampled parameters will be correlated. To reduce redundancy, $\beta_1 + \beta_2$ can be redefined as $\beta_0$. In more complex, hierarchical models, parameter redundancy is not always intuitive and can be solved by increasing the number of parameters [3, 4], but can be checked using various diagnostics. One simple diagnostic is to check for parameter correlations.

Using the MCMC sampling framework, we can check for parameter correlations within each model case to help us identify possible redundancy in parameters [1, 2]. If the sampled parameters are identifiable and non-redundant, we would see no correlation between parameters in both the full MCMC sampling case and the Laplace approximation case.

Note this is unnecessary for the AI Pcod model, since there is only one parameter estimated in that model.

```{r parameter correlation plot, echo=FALSE, message=FALSE, warning=FALSE, results='hide',fig.keep='all'}

PerformanceAnalytics::chart.Correlation(thrn_comp_log_tau[[2]] %>% filter(type == "la") %>% select(c(1:6))) # looking at correlations between parameter samples

PerformanceAnalytics::chart.Correlation(thrn_comp_log_tau[[2]] %>% filter(type == "mcmc") %>% select(c(1:6)))

```

**Fig 4a: Parameter correlations using the Laplace approximation, thornyhead rockfish model.** The diagonal gives the distribution of each sampled parameter. The lower off-diagonal gives the pairwise correlations between parameter samples, and the upper off-diagonal gives the statistical summary of parameter correlations (correlation value, $r$, in text; significance given by \* in panels). Figure constructed using the parameter samples when the Laplace approximation is assumed.

**Fig 4b: Parameter correlations using full MCMC sampling, thornyhead rockfish model.** Figure layout as in Fig 4a, but now using parameter samples from full MCMC sampling (i.e., no Laplace approximation).

In both cases, there are several parameters which are correlated with each other, such as process errors and log_tau_cpue. This suggests that some of the parameters might be redundent with each other, and that either (1) more data is needed to fit the complex model and/or (2) the model should be simplified to reduce parameter redundancy and increase parameter uniqueness (e.g., methods in [7]).

### 5. Residual analysis

Residuals are used to test the underlying assumptions about the error distributions in the model. For example, in a linear regression, residuals should be independent, normal, and have constant variance. Traditional residuals (e.g., Pearson's residuals) are inappropriate for state-space models like REMA, because process error variance may be over-estimated in cases where the model is mis-specified, thus leading to artificially small residuals (see Section 3 of Thygesen et al. 2017 for an example). The appropriate residual type for validation of state-space models are called one-step ahead (OSA) residuals. Instead of comparing the observed and expected values at the same time step, OSA residuals use forecasted values based on all previous observations (i.e., they exclude the observed value at the current time step from prediction). In this way, OSA residuals account for non-normality and correlation among years. Under a correctly specified model, resultant OSA residuals should be independent and identically distributed (i.i.d.) with a standard normal distribution $N(0,1)$.

Methods for calculating OSA residuals in REMA have been implemented in the `rema::get_osa_residuals()` using the `TMB::oneStepPredict()` function and the `fullGaussian` method. The `rema::get_osa_residuals()` returns tidied dataframes of observations, REMA model predictions, and OSA residuals for the biomass and CPUE survey data when appropriate, along with several diagnostic plots.

One useful diagnostic plot provided in the output to `rema::get_osa_residuals()` is the normal quantile-quantile (QQ) plot, which can help users determine if the OSA residuals are likely to have come from the expected standard normal distribution. In a normal QQ plot, the quantile values of the standard normal distribution are plotted on the x-axis, and the corresponding quantile values of the OSA residuals are plotted on the y-axis. If the OSA residuals are normally distributed, the points will fall on the 0/1 reference line. If the residuals are not normally distributed, the points will deviate from the reference line. The OSA residuals are plotted for the entire data set combined `$plots$qq` and by stratum for the biomass survey `$plots$biomass_qq` or CPUE survey `$plots$cpue_qq`. Although sample sizes within strata are often too small to have statistical power, it can also useful to check for residual patterns by stratum. In the top-left corner of the combined QQ plot `$plots$qq`, we provide the standard deviation of normalized residuals (SDNR) statistic. Given that OSA residuals should be $N(0,1)$ under a correctly specified model, we should expect the SDNR to be equal to close to 1.

We will examine visual fits of the OSA residuals by year in order to test for randomness or independence of the residuals (i.e., they should not be correlated by year). Standard approaches such as autocorrelation function (ACF) plots or residual runs tests (e.g., Wald-Wolfowitz test) are inappropriate for many REMA applications because of missing years of data in the survey time series.

```{r echo=TRUE, message=FALSE, warning=FALSE}

# run OSAs
pcod_resid <- get_osa_residuals(pcod_mod)

# Pcod plots
cowplot::plot_grid(pcod_resid$plots$qq + 
                     ggtitle('AI Pacific cod') + 
                     theme(legend.position = 'none'), pcod_resid$plots$biomass_resids, ncol = 1)

```

**Fig 5** The normal QQ plot plot for AI Pcod (top) suggests slight negative skewness in the residuals, though the small sample size makes interpretation challenging. The SDNR is 0.99 (a perfect model would have an SDNR=1.00), which means assumptions are likely not violated for the residuals. The plot of OSA residuals by year (bottom) shows no patterns in the residuals, suggesting they are independent.

```{r echo=TRUE, message=FALSE, warning=FALSE}
thrn_resid <- get_osa_residuals(thrn_mod)
thrn_resid$plots$qq + 
  ggtitle('GOA thornyheads') + 
  theme(legend.position = 'none')
# thrn_resid$plots$histo
thrn_resid$plots$biomass_qq + ggtitle('GOA thornyheads QQ plot for biomass strata')# biomass data
thrn_resid$plots$cpue_qq + ggtitle('GOA thornyheads QQ plot for CPUE strata')# cpue data
thrn_resid$plots$biomass_resids + ggtitle('GOA thornyheads biomass strata')
thrn_resid$plots$cpue_resids + ggtitle('GOA thornyheads CPUE strata')
```

**Fig 6** The combined normal QQ plot plot for the GOA thornyheads OSA residuals suggests they follow a normal distribution (the points fall along the 0/1 line), though there is evidence of slight positive, or right skewness (the majority of the points falling above to 0/1 line). The SDNR is exactly 1, indicating the variance assumptions are likely met for the residuals. The QQ plots for the biomass by strata highlight where some of the positive skewness may be coming from (e.g., EGOA 701-1000m, WGOA 0-500 m); however, small sample sizes by stratum make interpretation of these QQ plots challenging. The QQ plots for the CPUE by strata are mostly normal, though there some evidence of light tails in the WGOA stratum. The plot of OSA residuals by year for the biomass strata show no patterns in the residuals, suggesting they are independent. However, there are runs in the residuals for the CPUE strata (especially in the CGOA and WGOA), which may be indicative of model misspecification or misfit.

### Why should we care?

The REMA model is used within the North Pacific Fishery Management Council to obtain exploitable biomass estimates for Tier 4 crab and Tier 5 groundfish and as a method to apportion Acceptable Biological Catches among management regions. It's primary purpose is to smooth noisy survey biomass estimates. If it's just a fancy average, why should we care about potential red flags raised through during model validation? In the case of the GOA thornyhead model, for example, it appears there is an issue with the estimation of the additional observation error, pointing to potential over-parameterization of the model and/or parameter redundancy. Because terminal year predictions from the REMA model are used for management and the estimation of additional observation error increases the "smoothness" of model predictions, the model issues issues revealed for GOA Thornyhead may warrant further evaluation.

### Recommendations

The model validation methods presented in this vignette are tools for stock assessment scientists to evaluate existing models and guide development of future models. From this analysis, we found the MCMC diagnostics and OSA residuals to be the most useful diagnostics for the REMA model, and we recommend they are explored for existing models using multivariate configurations with multiple process errors, multi-survey versions using CPUE indices like the longline survey, and models estimating additional observation error. Additionally, we recommend these model diagnostics be used when recommending new models for management.

### References

<https://apps-afsc.fisheries.noaa.gov/Plan_Team/2023/AIpcod.pdf>

<https://discourse.mc-stan.org/t/preservation-of-mcmc-sample-results/15252/3>

<https://backend.orbit.dtu.dk/ws/files/163017606/Validation_EES.pdf>

[1] Auger‐Méthé, M., Newman, K., Cole, D., Empacher, F., Gryba, R., King, A. A., Leos-Barajas, J. ´ Mills Flemming, A. Nielsen, G. Petris, & Thomas, L. (2021). A guide to state–space modeling of ecological time series. Ecological Monographs, 91(4), e01470.

[2] Auger-Méthé, M., Field, C., Albertsen, C.M., Derocher, A.E., Lewis, M.A., Jonsen, I.D., Flemming, J.M. 2016. State-space models’ dirty little secrets: even simple linear Gaussian models can have estimation problems. Scientific reports, 6(1), 26677.

[3] Gimenez, O., Viallefont, A., Catchpole, E. A., Choquet, R., & Morgan, B. J. (2004). Methods for investigating parameter redundancy. Animal Biodiversity and Conservation, 27(1), 561-572.

[4] Cole, D. J. (2019). Parameter redundancy and identifiability in hidden Markov models. Metron, 77(2), 105-118.

[5] Skaug, H. J., & Fournier, D. A. (2006). Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models. Computational Statistics & Data Analysis, 51(2), 699-709.

[6] Monnahan, C. C., & Kristensen, K. 2018. No-U-turn sampling for fast Bayesian inference in ADMB and TMB: Introducing the adnuts and tmbstan R packages. PloS one, 13(5), e0197954.

[7] Campbell, D., & Lele, S. 2014. An ANOVA test for parameter estimability using data cloning with application to statistical inference for dynamic systems. Computational Statistics & Data Analysis, 70, 257-267.
